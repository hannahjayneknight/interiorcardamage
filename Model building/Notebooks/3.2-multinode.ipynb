{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi node parallel computing\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/ddp_series_multinode.html\n",
    "\n",
    "### A note on rank, local rank, and world size\n",
    "\n",
    "Suppose we run our training in 2 servers (some articles also call them nodes) and each server/node has 4 GPUs. The world size is 4*2=8. The ranks for the processes will be [0, 1, 2, 3, 4, 5, 6, 7]. In each node, the local rank will be [0, 1, 2, 3].\n",
    "\n",
    "You can think of world as a group containing all the processes for your distributed training. Usually, each GPU corresponds to one process. Processes in the world can communicate with each other, which is why you can train your model distributedly and still get the correct gradient update. So world size is the number of processes for your training, which is usually the number of GPUs you are using for distributed training.\n",
    "\n",
    "Rank is the unique ID given to a process, so that other processes know how to identify a particular process. Local rank is the a unique local ID for processes running in a single node.\n",
    "\n",
    "![](rank.png)\n",
    "\n",
    "### Note on rendezvous backend\n",
    "For multi-node training you need to specify:\n",
    "\n",
    "- --rdzv-id: A unique job id (shared by all nodes participating in the job)\n",
    "\n",
    "- --rdzv-backend: An implementation of torch.distributed.elastic.rendezvous.RendezvousHandler\n",
    "\n",
    "- --rdzv-endpoint: The endpoint where the rendezvous backend is running; usually in form host:port.\n",
    "\n",
    "Currently c10d (recommended), etcd-v2, and etcd (legacy) rendezvous backends are supported out of the box. To use etcd-v2 or etcd, setup an etcd server with the v2 api enabled (e.g. --enable-v2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
