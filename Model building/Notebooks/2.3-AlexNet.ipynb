{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet Data augmentation\n",
    "\n",
    "Same data augmentation as described in the original AlexNet paper.\n",
    "\n",
    "### Image translations and horizontal reflections\n",
    "\n",
    "Input image sizes before data augmentation are (256, 256).\n",
    "\n",
    "Random (224, 224) patches are extracted along with their horizontal reflections.\n",
    "\n",
    "**How does that transform work on multiple items?** They work on multiple items through use of the data loader. By using transforms, you are specifying what should happen to a single emission of data (e.g., batch_size=1). The data loader takes your specified batch_size and makes n calls to the ```__getitem__``` method in the torch data set, applying the transform to each sample sent into training/validation. It then collates n samples into your batch size emitted from the data loader. (Source: https://stackoverflow.com/questions/66370250/how-does-pytorch-dataloader-interact-with-a-pytorch-dataset-to-transform-batches) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the transformer for AlexNet\n",
    "transformer=transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5],\n",
    "                        [0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Altering intensities of RGB channels\n",
    "\n",
    "AlexNet also alters the RGB intensities in training images. After finding the principle components (after PCA) in the whole image dataset, they perform eigenvector x eigenvalue x random variable and add this to each input image.\n",
    "\n",
    "However, this only reduces error by 1%, it isn't common practice and hasn't been seen in other implementations of AlexNet - is it necessary?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check these pca values from imagenet? Or find for my own dataset?\n",
    "__imagenet_pca = {\n",
    "    'eigval': torch.Tensor([0.2175, 0.0188, 0.0045]),\n",
    "    'eigvec': torch.Tensor([\n",
    "        [-0.5675,  0.7192,  0.4009],\n",
    "        [-0.5808, -0.0045, -0.8140],\n",
    "        [-0.5836, -0.6948,  0.4203],\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Lighting data augmentation take from here - https://github.com/eladhoffer/convNet.pytorch/blob/master/preprocess.py\n",
    "class Lighting(object):\n",
    "    \"\"\"Lighting noise(AlexNet - style PCA - based noise)\"\"\"\n",
    "\n",
    "    def __init__(self, alphastd, eigval, eigvec):\n",
    "        self.alphastd = alphastd\n",
    "        self.eigval = eigval\n",
    "        self.eigvec = eigvec\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if self.alphastd == 0:\n",
    "            return img\n",
    "\n",
    "        alpha = img.new().resize_(3).normal_(0, self.alphastd)\n",
    "        rgb = self.eigvec.type_as(img).clone()\\\n",
    "            .mul(alpha.view(1, 3).expand(3, 3))\\\n",
    "            .mul(self.eigval.view(1, 3).expand(3, 3))\\\n",
    "            .sum(1).squeeze()\n",
    "        return img.add(rgb.view(3, 1, 1).expand_as(img))\n",
    "\n",
    "transformer = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(256),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            Lighting(0.1, __imagenet_pca['eigval'], __imagenet_pca['eigvec']),\n",
    "            transforms.Normalize([0.5,0.5,0.5],\n",
    "                        [0.5,0.5,0.5])\n",
    "        ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an augmentation policy\n",
    "\n",
    "https://pytorch.org/vision/stable/generated/torchvision.transforms.AutoAugment.html#torchvision.transforms.AutoAugment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model\n",
    "\n",
    "The original AlexNet as written in the paper can be found here: https://github.com/dansuh17/alexnet-pytorch/blob/master/model.py \n",
    "\n",
    "The PyTorch implementation (found here: https://github.com/pytorch/vision/blob/main/torchvision/models/alexnet.py) uses a single GPU version of the original AlexNet (https://github.com/pytorch/vision/pull/463)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
